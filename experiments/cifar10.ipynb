{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from regularizers.regularizer import L1Regularizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> /utils/data_utils.py\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> utils/evaluation_utils.py\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> /models/model.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Net\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATE TRAINING LOOP\n",
    "\n",
    "loss_across_epochs = []\n",
    "latency_across_epochs = []\n",
    "\n",
    "def training_loop(optimizer,inputs,labels,model,regularizer_loss=None) -> \"loss\":\n",
    "    # get the inputs\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # REGULARIZATION\n",
    "    if regularizer_loss:\n",
    "        loss = regularizer_loss.regularized_all_param(reg_loss_function=loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_utils import training_loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_regularizers = 2\n",
    "\n",
    "list_of_models = [Net()]*num_of_regularizers\n",
    "\n",
    "list_of_regularizers = [None,L1Regularizer(model=list_of_models[1], lambda_reg=10**-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "\n",
    "def calculate_FLOPS(model, input):\n",
    "\n",
    "    flops, params  = profile(model,  inputs=input)\n",
    "\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FLOPS FUNCTIONALITY\n",
    "\n",
    "# 1 training cycle for ALL regularizers\n",
    "# 1 epoch, 1 data point\n",
    "# for every epoch:\n",
    "# for every data point:\n",
    "\n",
    "data1 = next(dataiter)\n",
    "# net.to(device)\n",
    "\n",
    "inputs, labels = data1\n",
    "# inputs, labels = inputs.to(device), labels.to(device) # GPU\n",
    "\n",
    "#print(inputs.shape) # shape of input 4 3 32 32 \n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model1 = list_of_models[0]\n",
    "\n",
    "model1.eval()\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "flops, params  = profile(model1,  inputs=(input_tensor,) )\n",
    "\n",
    "print(\"Flops\" , flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_latency_for_all_regularizers(inputs,labels):\n",
    "\n",
    "    loss_across_regularizers = []\n",
    "    latency_across_regularizers = []\n",
    "\n",
    "    for regularizer_index,regularizer in enumerate(list_of_regularizers):\n",
    "        #time each training loop to get latency\n",
    "        start = time.time()\n",
    "        loss = training_loop(inputs,labels,regularizer)\n",
    "        end = time.time()\n",
    "\n",
    "        loss_across_regularizers.append(loss.item())\n",
    "        latency_across_regularizers.append(end - start)\n",
    "\n",
    "    loss_across_epochs.append(loss_across_regularizers)\n",
    "    latency_across_epochs.append(latency_across_regularizers)\n",
    "    # print(loss_across_regularizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> utils/training_utils.py\n",
    "# net.to(device)\n",
    "\n",
    "\n",
    "# TRAINING CHANNEL\n",
    "\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # limit training time for debugging purposes\n",
    "        if i > 5:\n",
    "            break\n",
    "\n",
    "        data1 = next(dataiter)\n",
    "        # net.to(device)\n",
    "\n",
    "        inputs, labels = data1\n",
    "\n",
    "        #since we are not interested in inference, loss and latency can be calculated in one training loop\n",
    "        calculate_loss_latency_for_all_regularizers(inputs,labels)\n",
    "        # calculate_FLOPS()\n",
    "        print(i)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "- [cifar10 (Colab)](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb#scrollTo=ubQdOyR6FjUB)\n",
    "\n",
    "- [Iterative Pruning (Colab)](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n",
    "\n",
    "- [Pruning Experiments (Github)](https://github.com/olegpolivin/pruningExperiments/tree/main)\n",
    "- [Pruning Experiments (Medium)](https://olegpolivin.medium.com/experiments-in-neural-network-pruning-in-pytorch-c18d5b771d6d)\n",
    "- [Sparsifying Regularizer (Github)](https://github.com/dizam92/pyTorchReg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components\n",
    "- data loading + preprocessing (/utils/data_utils.py ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> utils/evaluation_utils.py\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Net\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATE TRAINING LOOP\n",
    "\n",
    "loss_across_epochs = []\n",
    "latency_across_epochs = []\n",
    "\n",
    "def training_loop(optimizer,inputs,labels,model,regularizer_loss=None) -> \"loss\":\n",
    "    # get the inputs\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # REGULARIZATION\n",
    "    if regularizer_loss:\n",
    "        loss = regularizer_loss.regularized_all_param(reg_loss_function=loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_regularizers = 2\n",
    "\n",
    "list_of_models = [Net()]*num_of_regularizers\n",
    "\n",
    "list_of_regularizers = [None,L1Regularizer(model=list_of_models[1], lambda_reg=10**-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FLOPS FUNCTIONALITY\n",
    "\n",
    "# 1 training cycle for ALL regularizers\n",
    "# 1 epoch, 1 data point\n",
    "# for every epoch:\n",
    "# for every data point:\n",
    "\n",
    "data1 = next(dataiter)\n",
    "# net.to(device)\n",
    "\n",
    "inputs, labels = data1\n",
    "# inputs, labels = inputs.to(device), labels.to(device) # GPU\n",
    "\n",
    "#print(inputs.shape) # shape of input 4 3 32 32 \n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model1 = list_of_models[0]\n",
    "\n",
    "model1.eval()\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "flops, params  = profile(model1,  inputs=(input_tensor,) )\n",
    "\n",
    "print(\"Flops\" , flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> utils/training_utils.py\n",
    "# net.to(device)\n",
    "\n",
    "\n",
    "# TRAINING CHANNEL\n",
    "\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # limit training time for debugging purposes\n",
    "        if i > 5:\n",
    "            break\n",
    "\n",
    "        data1 = next(dataiter)\n",
    "        # net.to(device)\n",
    "\n",
    "        inputs, labels = data1\n",
    "\n",
    "        #since we are not interested in inference, loss and latency can be calculated in one training loop\n",
    "        calculate_loss_latency_for_all_regularizers(inputs,labels)\n",
    "        # calculate_FLOPS()\n",
    "        print(i)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PLOTTING FIGURES ########\n",
    "\n",
    "# -> utils/plotting_utils.py\n",
    "columns=[\"No Regularizer\",\"L1 Regularizer\"]\n",
    "df = pd.DataFrame(loss_across_epochs, columns=columns)\n",
    "print(df)\n",
    "plot = df.plot(title=\"Regularizer vs Loss\")\n",
    "plot.set(xlabel=\"Time/Iteration\", ylabel=\"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
